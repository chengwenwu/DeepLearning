<center><h1>线性单元和梯度下降</h1></center>

学习[这篇文章](<https://www.zybuluo.com/hanbingtao/note/448086>)后的笔记。

- **线性模型**：输出是输入的线性组合。
- 在有标记的样本非常少的情况下会使用**无监督学习**。
- **目标函数**：其实就是计算误差的函数，由于到最后，误差只和模型的参数有关，所以目标函数最终是关于模型参数的函数，我们的目标是最小化它的值。

- **梯度下降优化算法**：不断的修改模型参数W的值，使其向目标函数负梯度方向运动。
  - **梯度**：**梯度**是一个向量，它指向**函数值上升最快**的方向。
- **随机梯度下降法(SDG)**:
  - 如果每次使用所有样本，将其输入模型，得到输出，然后更新一次w，计算量就会非常大（批梯度下降）。因此比较实用的算法是随机梯度下降法。
  - 随机梯度下降法，每次选择一个样本，输入模型，得到输出，然后立马更新W，由于随机选择的样本可能具有噪音，所以w可能并不是每次都向E的负梯度方向前进，但是总体上它在向E的负梯度方向前进，最终还是可以收敛。它比SDG的话效率更高，并且有可能这种随机性是一件好事，对一些非凸函数，可能存在局部最小值，随机梯度下降，有可能跳出那个不是我们所期望的极小值，而找到我们期望的极小值。